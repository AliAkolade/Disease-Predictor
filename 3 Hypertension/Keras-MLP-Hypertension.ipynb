{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Tensorflow(Keras) \n",
    "\n",
    "Sequential Multi Layer Percepton (MLP) neural network built using Keras, containing: \n",
    "\n",
    "\n",
    "### Model Flow \n",
    "\n",
    "### Data Preprocessing\n",
    "- collect and explore data\n",
    "- normalize input features \n",
    "- split into 5 folds cross validation train/test datasets \n",
    "    \n",
    "   \n",
    "### Model Construction \n",
    "- Utilize Kera's sequential class to construct input, hidden, and output layers. Model consists of performaning matrix multiplications during each level and producing an activation output using mathematical functions. \n",
    "    \n",
    "- Input layer: \n",
    "    - Takes feature inputs and intialize with random weights and biases \n",
    "    - Utilize activation function (ReLU, tanh, etc) to produce inputs for hidden layers\n",
    "        \n",
    "- Hidden Layers:\n",
    "    - Decide on number of hidden layers and different parameters \n",
    "    - Takes our input data and performs mathematical operations to produce inputs to output layer\n",
    "\n",
    "- Output Layer: \n",
    "    - Final step in neural network should produce a probability between 0-1 to classify \n",
    "        \n",
    "   \n",
    "### Model training \n",
    "- We can train (fit) our data using the constructed model. The network will iterativly(number of epochs) train and try to improve it's performance. Uses an optimizer in attempt to reduce the training loss, process know as gradiant descent, and achieve higher accuracy. \n",
    "    \n",
    "- Loss Function:\n",
    "    - Since this is a binary classification problem, use binary_crosscentropy to calculate the loss function between predicted and actual output\n",
    "        \n",
    "- Optimization: \n",
    "    - We optimize the neural network with an Adam optimizer.\n",
    "    - Adam = Adaptive moment estimation, combination of RMSProp + Momentum\n",
    "        \n",
    "- Stochastic Gradiant Descent: \n",
    "    - Momentum takes past gradiants into account to smooth out the gradiant descent\n",
    "    \n",
    "    \n",
    "### Model Evaluation \n",
    "- Confusing Matrix: Using Sklearn metrics library\n",
    "\n",
    "    True Negative     False Positive \n",
    "    False Negative    True Positive \n",
    "    \n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    - Accuracy needs to be considered depending on the problem\n",
    "    \n",
    "- Recall: TP / (TP + FN)\n",
    "    - High recall indicates the class is correctly recognized\n",
    "    - E.g. small number of false negatives\n",
    "    \n",
    "- Precision: TP / (TP + FP)\n",
    "    - High precision indicates an example labled as positive \n",
    "        is indeed positive \n",
    "    \n",
    "- F-Score: 2*recall*precision / (recall + precision)\n",
    "    - F-score helps measure recall and precision at same time\n",
    "    - It takes harmonic mean instead of arithmitic mean\n",
    "\n",
    "\n",
    "### Area Under the Curve (AUC): Performance Measurement \n",
    "\n",
    "- calculating the AUC using the trapezoidal rule for the \n",
    "    ROC-curve\n",
    "- ROC is a probability curve and AUC represents degree of separability\n",
    "- Shows how much a model is capable of distinguishing between classes\n",
    "    - Higher AUC, better at predicting 0 as 0, 1 as 1\n",
    "    - E.g. better at disease diagnosis\n",
    "\n",
    "- TPR (True pos rate) / Recall / Sensitivity = TP / (TP+FN)\n",
    "- FPR (False pos rate) = 1 - Specificity\n",
    "- Specificity = TN / (TN + FP)\n",
    "\n",
    "- Sensitivity and Specificity are inversely proportional to each other\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Data preprocessing \n",
    "df = pd.read_csv(\"Data\\Hypertension.csv\")\n",
    "\n",
    "indexedData = df[['SEX', 'WC', 'SBP', 'DBP', 'HTN']]\n",
    "cols_to_norm = [ 'WC', 'SBP', 'DBP']\n",
    "indexedData = indexedData.dropna()\n",
    "indexedData.info()\n",
    "data = indexedData\n",
    "\n",
    "X = data[['SEX', 'WC', 'SBP', 'DBP']]\n",
    "Y = data[['HTN']]\n",
    "indexedData.head()\n",
    "indexedData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing input features \n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X), columns = X.columns, index = X.index)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits =5, shuffle = True, random_state = seed)\n",
    "cvscores = []\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Constructing the layers of our neural network\n",
    "Kernel is the weight matrix, kernel initialization sets up \n",
    "     the initial random weights of the keras layers \n",
    "   \n",
    "The output layer uses a sigmoid activation since it's a \n",
    "    binary classification \n",
    "    \n",
    "K-Folds Cross Validation:\n",
    "    Method of testing robustness of model by running the model K times and using the average\n",
    "\"\"\"\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    train_X, valid_X = X.iloc[train], X.iloc[test]\n",
    "    train_Y, valid_Y = Y.iloc[train], Y.iloc[test]\n",
    "    \n",
    "    classifier = Sequential()\n",
    "    #Input layer\n",
    "    classifier.add(Dense(4,kernel_initializer ='random_uniform', \n",
    "                         kernel_regularizer=keras.regularizers.l2(l=0.01), \n",
    "                         input_dim = 4))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Activation(activation='relu'))\n",
    "    \n",
    "    #First hidden layer\n",
    "    classifier.add(Dense(8,kernel_initializer='random_uniform',\n",
    "                         kernel_regularizer=keras.regularizers.l2(l=0.01)))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Activation(activation='relu'))\n",
    "    \n",
    "    #Output layer\n",
    "    classifier.add(Dense(1, kernel_initializer='random_uniform', \n",
    "                         kernel_regularizer=keras.regularizers.l2(l=0.01)))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Activation(activation='sigmoid'))\n",
    "    \n",
    "    #Model training and optimization \n",
    "    adam = optimizers.adam(lr = .001)\n",
    "    classifier.compile(loss='binary_crossentropy', optimizer=adam ,metrics = ['accuracy'])   \n",
    "    classifier.fit(train_X, train_Y, batch_size=32, epochs= 15, verbose = 0)\n",
    "\n",
    "    eval_model = classifier.evaluate(valid_X, valid_Y)\n",
    "    print(\"%s: %.2f%%\" % (classifier.metrics_names[1], eval_model[1]*100), verbose = 0)\n",
    "    cvscores.append(eval_model[1] * 100)\n",
    "                                 \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = classifier.predict(valid_X)\n",
    "y_predict = (y_predict > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Producing confusing matrix and report \n",
    "\n",
    "Confusing Matrix: \n",
    "    True Negative     False Positive \n",
    "    False Negative    True Positive \n",
    "    \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    - Accuracy needs to be considered depending on the problem\n",
    "Recall: TP / (TP + FN)\n",
    "    - High recall indicates the class is correctly recognized\n",
    "    - E.g. small number of false negatives\n",
    "Precision: TP / (TP + FP)\n",
    "    - High precision indicates an example labled as positive \n",
    "        is indeed positive \n",
    "F-Score: 2*recall*precision / (recall + precision)\n",
    "    - F-score helps measure recall and precision at same time\n",
    "    - It takes harmonic mean instead of arithmitic mean\n",
    "\n",
    "\"\"\"\n",
    "print(metrics.accuracy_score(valid_Y, y_predict))\n",
    "print(confusion_matrix(valid_Y, y_predict))\n",
    "print(classification_report(valid_Y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['HTN']\n",
    "cm = confusion_matrix(valid_Y, y_predict)\n",
    "print(cm)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Area Under the Curve (AUC): Performance Measurement \n",
    "- calculating the AUC using the trapezoidal rule for the \n",
    "    ROC-curve\n",
    "- ROC is a probability curve and AUC represents degree of separability\n",
    "- Shows how much a model is capable of distinguishing between classes\n",
    "    - Higher AUC, better at predicting 0 as 0, 1 as 1\n",
    "    - E.g. better at disease diagnosis\n",
    "\n",
    "TPR (True pos rate) / Recall / Sensitivity = TP / (TP+FN)\n",
    "FPR (False pos rate) = 1 - Specificity\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "Sensitivity and Specificity are inversely proportional to each other\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "# preds = y_pred[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(valid_Y, y_predict)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d429be2f",
   "language": "python",
   "display_name": "PyCharm (Diseases Predictor)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}